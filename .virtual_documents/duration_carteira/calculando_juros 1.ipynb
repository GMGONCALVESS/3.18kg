# import libraries
import pandas as pd
import time
import xlrd
import requests
import csv
import math
import numpy as np
from sqlalchemy import create_engine, MetaData, Table
from datetime import datetime, date, timedelta
from urllib.request import urlretrieve
from pandas.tseries.holiday import AbstractHolidayCalendar, Holiday
from pandas.tseries.offsets import CustomBusinessDay
from openpyxl import Workbook
import math



# def truncate(value, decimals=16):
#     factor = 10 ** decimals
#     return math.trunc(value * factor) / factor
def truncate(value, decimals=16):
    if isinstance(value, pd.DataFrame):
        # Aplicar apenas em colunas numéricas
        numeric_cols = value.select_dtypes(include=[np.number]).columns
        value[numeric_cols] = value[numeric_cols].apply(lambda col: truncate(col, decimals))
        return value

    if isinstance(value, (pd.Timestamp, pd.DatetimeIndex, np.datetime64, pd.Series)) and pd.api.types.is_datetime64_any_dtype(value):
        return value  # Retorna datetime sem modificações

    factor = 10 ** decimals
    return math.trunc(value * factor) / factor

# Pega feriados nacionais pelo calendário da Anbima
def holidays(url=None, path=None):
    if not url:
        url = 'http://www.anbima.com.br/feriados/arqs/feriados_nacionais.xls'
    if not path:
        path = 'feriados_nacionais.xls'
    try:
        wb = xlrd.open_workbook(path)
    except:
        response = urlretrieve(url, filename=path)
        wb = xlrd.open_workbook(path)
    ws = wb.sheet_by_index(0)
    i = 1
    dates = []
    while ws.cell_type(i, 0) == 3:
        y, m, d, _, _, _ = xlrd.xldate_as_tuple(
            ws.cell_value(i, 0), wb.datemode)
        dates.append(date(y, m, d))
        i += 1
    return dates

# Cria calendário de feriados nacionais
class CustomBusinessCalendar(AbstractHolidayCalendar):
    rules = [Holiday('Brazil Holiday', month=date.month,
                     day=date.day, year=date.year) for date in holidays()]


def get_busdays(start_date, end_date):
    # Cria calendário de dias úteis
    custom_business_day = CustomBusinessDay(calendar=CustomBusinessCalendar())
    dates = pd.bdate_range(start=start_date, end=end_date,
                           freq=custom_business_day)
    return len(dates)

# Define the function to swap the rows


def swap_rows(df):
    # Iterate over the DataFrame by index
    i = 0
    while i < len(df) - 1:
        # Check if the 'evento' column has the specific values in the current row
        if df.at[i, 'evento'] in ['Amortizacao', 'Vencimento (resgate)']:
            # Swap the current row with the next one
            df.iloc[i], df.iloc[i+1] = df.iloc[i+1].copy(), df.iloc[i].copy()
            # Move the index by 2 to skip the next row (since it's already swapped)
            i += 1
        i += 1
    return df


def adjust_pu(row, previous_pu):
    if row['evento'] in ['Amortizacao', 'Vencimento (resgate)', 'Resgate total antecipado']:
        if row['codigo_ticker'] == 'SNGO18' and row['evento'] == 'Amortizacao':
            # print(f'percentual_taxa:{row['percentual_taxa']}; vne:{row['vne']}')
            valor_pago = (row['percentual_taxa']/100.0) * row['vne']
            return previous_pu - valor_pago
        else:
            # Adjust using the previous 'pu'
            return previous_pu * (1 - row['percentual_taxa'] / 100)
       # return previous_pu * (1 - row['percentual_taxa'] / 100)  # Adjust using the previous 'pu'
    else:
        return previous_pu  # Use the previous 'pu' for other events without adjustment

# Step 1: Create a function to find the closest tenor


def get_closest_tenor(days, tenores_dict):
    # Find the tenor with the minimum difference to 'days'
    closest_tenor = min(tenores_dict, key=lambda k: abs(k - days))
    return closest_tenor


def calcular_juros_acimadi(vne, spread_anual, dp, start_date, end_date):
    # Constante: 252 dias úteis no ano
    dias_uteis_ano = 252

    tdi_k_diario = pd.DataFrame()

    # Seleciona o período do DI
    tdi_k_diario = indicador_cdi[(indicador_cdi['index'] >= start_date) & (
        indicador_cdi['index'] < end_date)]

    tdi_k_diario.to_excel('tdi_k_diario.xlsx')

    # Calcular a taxa DI diária
    tdi_k_diario.loc[:, 'tdi_k_diario'] = (
        (1 + tdi_k_diario['px_last']/100) ** (1/dias_uteis_ano) - 1)
    # tdi_k_diario.loc[:, 'tdi_k_diario'] = tdi_k_diario['tdi_k_diario'] + 1

    print(tdi_k_diario)
    print(type(tdi_k_diario))

    tdi_k_diario = truncate(tdi_k_diario, 16)
    
    # Calcular o fator DI
    spread_anual = round(spread_anual, 2)
    fator_di = np.prod(1 + (tdi_k_diario['tdi_k_diario'] * (spread_anual/100)))

    fator_di = round(fator_di, 8)
    
    # Calcular os juros
    juros = vne * (fator_di - 1)

    print({
        'Fator DI': fator_di,
        'Juros Pagos': juros
    })

    return juros


def calcular_juros(vne, spread_anual, dp, start_date, end_date):
    # Constante: 252 dias úteis no ano
    dias_uteis_ano = 252

    vne = truncate(vne, 6)
    
    tdi_k_diario = pd.DataFrame()

    # Seleciona o período do DI
    tdi_k_diario = indicador_cdi[(indicador_cdi['index'] >= start_date) & (
        indicador_cdi['index'] < end_date)]

    tdi_k_diario.to_excel('tdi_k_diario.xlsx')

    # Calcular a taxa DI diária
    tdi_k_diario.loc[:, 'tdi_k_diario'] = (
        1 + tdi_k_diario['px_last']/100) ** (1/dias_uteis_ano) - 1
    tdi_k_diario.loc[:, 'tdi_k_diario'] = tdi_k_diario['tdi_k_diario'] + 1

    print(tdi_k_diario)
    print(type(tdi_k_diario))

    tdi_k_diario = truncate(tdi_k_diario, 16)
    
    # Calcular o fator DI
    fator_di = np.prod(tdi_k_diario['tdi_k_diario'])
    spread_anual = round(spread_anual, 4)
    
    fator_di = round(fator_di, 8)
    
    # Calcular o fator spread
    fator_spread = (1 + spread_anual / 100) ** (dp / dias_uteis_ano)
    
    # Calcular o fator de juros
    fator_juros = fator_di * fator_spread

    fator_juros = round(fator_juros, 9)
    
    # Calcular os juros
    juros = vne * (fator_juros - 1)

    print({
        'Fator DI': fator_di,
        'Fator Spread': fator_spread,
        'Fator Juros': fator_juros,
        'Juros Pagos': juros
    })

    return juros

# Function to calculate juros_pagos only for 'Pagamento de Juros' events


def apply_calcular_juros(row):
    if (row['evento'] == 'Pagamento de juros'):
        # Call the calcular_juros function using the appropriate columns
        dp = row['dp'] - 1
        print(f'evento:{row['evento']}; vne:{row['pu']}; percentual_taxa:{row['percentual_taxa']}; dp:{dp}; start_date:{row['start_date']}; end_date:{row['end_date']}')
        if row['percentual_taxa'] >= 100:
            return calcular_juros_acimadi(row['pu'], row['percentual_taxa'], dp, row['start_date'], row['end_date'])
        else:
            return calcular_juros(row['pu'], row['percentual_taxa'], dp, row['start_date'], row['end_date'])
    elif row['evento'] in ['Amortizacao', 'Vencimento (resgate)', 'Resgate total antecipado']:
        print(f'evento: {row['evento']}; pu:{row['pu']}')
        return row['pu']
    return None  # Return None for rows that don't match the condition


def update_valor_recebido(row, previous_row):
    # Check if the event is 'Resgate total antecipado' or 'Vencimento (resgate)'
    if row['evento'] in ['Resgate total antecipado', 'Vencimento (resgate)']:
        # Get the previous row's 'pu' and multiply by 'quantidade'
        return previous_row['pu'] * row['quantidade'] if pd.notnull(previous_row['pu']) else None
    # Return the original valor_recebido for other rows
    return row['valor_recebido']





#DEMORADO
# Example: 'postgresql://username:password@localhost:5432/your_database'
engine = create_engine(
    'postgresql://postgres:admin@192.168.88.61:5432/posicoesdb')

# sql query to read all the records
posicoes_query = pd.read_sql(
    'SELECT * FROM posicoes_pbi ORDER BY posicao_id', engine)

# convert the SQL table into a pandas dataframe
posicoes_pbi = pd.DataFrame(posicoes_query)

# sql query to read all the records
eventos_query = pd.read_sql(
    'SELECT * FROM eventos_debenture ORDER BY deb_id', engine)

# convert the SQL table into a pandas dataframe
eventos_debenture = pd.DataFrame(eventos_query)

# sql query to read all the records
eventos_cricra_query = pd.read_sql(
    'SELECT * FROM eventos_cricra ORDER BY cricra_id', engine)

# convert the SQL table into a pandas dataframe
eventos_cricra = pd.DataFrame(eventos_cricra_query)

# sql query to read all the records
curvadi_query = pd.read_sql('SELECT * FROM curva_di ORDER BY di_index', engine)

# convert the SQL table into a pandas dataframe
curva_di = pd.DataFrame(curvadi_query)
curva_di = pd.read_excel("curvadi_1902.xlsx")


start_date = '2019-02-26'
end_date = '2050-08-26'

# Cria calendário de dias úteis
custom_business_day = CustomBusinessDay(calendar=CustomBusinessCalendar())
dates = pd.bdate_range(start=start_date, end=end_date,
                       freq=custom_business_day)

eventos_debenture_cdi = eventos_debenture[eventos_debenture['indice'] == 'DI']




# eventos_debenture_cdi[eventos_debenture['codigo_ticker'] == 'ALGA28']
eventos_debenture_cdi.reset_index(drop=True, inplace=True)
eventos_debenture_cdi['deb_id'] = eventos_debenture_cdi.index

# Apply the swap function
eventos_debenture_cdi = swap_rows(eventos_debenture_cdi)

# Update the 'percentual_taxa' column to 100 where 'evento' is 'Vencimento (resgate)' or 'Resgate total antecipado'
eventos_debenture_cdi.loc[eventos_debenture_cdi['evento'].isin(
    ['Vencimento (resgate)', 'Resgate total antecipado']), 'percentual_taxa'] = 100

# eventos_debenture_cdi['pu'] = 1000.0
eventos_debenture_cdi['pu'] = eventos_debenture_cdi['vne']
# Adjust 'pu' using the previous 'pu' value for 'Amortizacao' or 'Vencimento (resgate)'

previous_codigo = None
previous_pu = None

# Iterate through the DataFrame
for index, row in eventos_debenture_cdi.iterrows():
    if previous_codigo is not None and previous_codigo != row['codigo_ticker']:
        # Set 'pu' to 1000.0 when a new 'codigo_ticker' is encountered
        # eventos_debenture_cdi.at[index, 'pu'] = 1000.0
        # previous_pu = 1000.0  # Reset previous_pu to 1000.0 for the new ticker
        eventos_debenture_cdi.at[index, 'pu'] = row['vne']
        # Reset previous_pu to 1000.0 for the new ticker
        previous_pu = row['vne']
    elif previous_pu is not None:
        # Adjust or propagate the previous 'pu'
        eventos_debenture_cdi.at[index, 'pu'] = adjust_pu(row, previous_pu)

    # Update the previous values for 'codigo_ticker' and 'pu'
    previous_codigo = row['codigo_ticker']
    # Store the updated 'pu'
    previous_pu = eventos_debenture_cdi.at[index, 'pu']


indicador_brasil_usa_query = pd.read_sql(
    "SELECT * FROM indicadores_brasil_usa WHERE pais = 'Brasil' ORDER BY indicador_id", engine)

indicador_brasil = pd.DataFrame(indicador_brasil_usa_query)

# sql query to read all the records
indicadorcdi_query = pd.read_sql(
    'SELECT * FROM indicador_cdi ORDER BY index', engine)

# convert the SQL table into a pandas dataframe
indicador_cdi = pd.DataFrame(indicadorcdi_query)


# Step 1: Convert the date columns to datetime if necessary
eventos_debenture_cdi['data_evento'] = pd.to_datetime(
    eventos_debenture_cdi['data_evento'])
indicador_cdi['index'] = pd.to_datetime(indicador_cdi['index'])
indicador_brasil['data'] = pd.to_datetime(indicador_brasil['data'])

# Step 2: Merge the DataFrames based on the date column
eventos_debenture_cdi = eventos_debenture_cdi.merge(
    indicador_cdi[['index', 'px_last']],  # Select only the necessary columns
    left_on='data_evento',  # Merge on the 'data_evento' column in eventos_debenture_cdi
    right_on='index',  # Merge on the 'index' column in indicador_cdi
    how='left'  # Keep all rows from eventos_debenture_cdi and match with indicador_cdi
)

# Step 3: Rename the 'px_last' column to 'cdi' and drop the redundant 'index' column
eventos_debenture_cdi = eventos_debenture_cdi.rename(
    columns={'px_last': 'cdi'})
eventos_debenture_cdi = eventos_debenture_cdi.drop(columns=['index'])

# Step 2: Merge the DataFrames based on the date column
eventos_debenture_cdi = eventos_debenture_cdi.merge(
    indicador_brasil[['data', 'cdi']],  # Select only the necessary columns
    left_on='data_evento',  # Merge on the 'data_evento' column in eventos_debenture_cdi
    right_on='data',  # Merge on the 'index' column in indicador_cdi
    how='left'  # Keep all rows from eventos_debenture_cdi and match with indicador_cdi
)
eventos_debenture_cdi = eventos_debenture_cdi.rename(columns={'cdi_y': 'cdi'})
eventos_debenture_cdi = eventos_debenture_cdi.drop(columns=['data', 'cdi_x'])

tenores = pd.read_excel('curvadi_1902.xlsx')

tenores = tenores.set_index('tenor')['bid_yield'].to_dict()


indicador_cdi['index'] = pd.to_datetime(indicador_cdi['index'])

max_date = eventos_debenture_cdi['data_evento'].max()
min_date = indicador_cdi['index'].max()

custom_business_day = CustomBusinessDay(calendar=CustomBusinessCalendar())
new_dates = pd.bdate_range(
    start=min_date, end=max_date, freq=custom_business_day)
new_dates = new_dates[1:]

new_entries = pd.DataFrame({
    'index': new_dates,
    'px_last': [float('nan')] * len(new_dates)
})




indicador_cdi


new_entries


# Append the new entries to indicador_cdi
indicador_cdi = pd.concat([indicador_cdi, new_entries], ignore_index=True)

# Sort the DataFrame by 'data' to maintain chronological order
indicador_cdi.sort_values(by='index', inplace=True)

# Reset index if necessary
indicador_cdi.reset_index(drop=True, inplace=True)




tenores


# import pandas as pd
# from IPython.core.display import display, HTML

# # Set the max height and enable scrolling
# def display_scrollable(df, max_height=400, max_width=1000):
#     display(HTML(df.to_html(classes="scroll-table")))

#     # Apply CSS to make it scrollable
#     display(HTML(f"""
#     <style>
#         .scroll-table {{
#             max-height: {max_height}px;
#             max-width: {max_width}px;
#             overflow: auto;
#             display: block;
#             white-space: nowrap;
#         }}
#     </style>
#     """))

# # Example usage
# display_scrollable(indicador_cdi)


indicador_cdi['days'] = (indicador_cdi['index'] - datetime.today()).dt.days


indicador_cdi['tenor'] = indicador_cdi['days'].apply(
    lambda x: get_closest_tenor(x, tenores))


indicador_cdi = indicador_cdi.merge(
    curva_di[['tenor', 'bid_yield']], on='tenor', how='left')


indicador_cdi['px_last'] = indicador_cdi['px_last'].fillna(
    indicador_cdi['bid_yield'])
indicador_cdi = indicador_cdi.drop(columns=['bid_yield'])


eventos_debenture_cdi['days'] = (
    eventos_debenture_cdi['data_evento'] - datetime.today()).dt.days

# Step 2: Apply the function to create a new 'tenor' column
eventos_debenture_cdi['tenor'] = eventos_debenture_cdi['days'].apply(
    lambda x: get_closest_tenor(x, tenores))



# First, perform the merge based on the 'tenor' column
eventos_debenture_cdi = eventos_debenture_cdi.merge(
    curva_di[['tenor', 'bid_yield']], on='tenor', how='left')



eventos_debenture_cdi['cdi'] = eventos_debenture_cdi['cdi'].fillna(
    eventos_debenture_cdi['bid_yield'])
eventos_debenture_cdi = eventos_debenture_cdi.drop(columns=['bid_yield'])



# First, create a new column for the 'end_date' by shifting 'data_evento' column up by one row
eventos_debenture_cdi['start_date'] = eventos_debenture_cdi['data_evento'].shift(
    1)
eventos_debenture_cdi['end_date'] = eventos_debenture_cdi['data_evento']

eventos_debenture_cdi.loc[eventos_debenture_cdi['start_date'].isnull(
), 'start_date'] = eventos_debenture_cdi['inicio_rentabilidade']

# Create a boolean Series where the current 'codigo_ticker' is different from the previous one
codigo_change = eventos_debenture_cdi['codigo_ticker'] != eventos_debenture_cdi['codigo_ticker'].shift(
    1)



#DEMORADO
# Use .loc to update 'start_date' where 'codigo_ticker' changes
eventos_debenture_cdi.loc[codigo_change,
                          'start_date'] = eventos_debenture_cdi['inicio_rentabilidade']

# Iterate over the rows of the DataFrame
for i in range(1, len(eventos_debenture_cdi)):  # Start from 1 to avoid accessing index -1
    # Check if start_date is equal to end_date
    if ((eventos_debenture_cdi.loc[i, 'start_date'] == eventos_debenture_cdi.loc[i, 'end_date']) & (eventos_debenture_cdi.loc[i, 'evento'] == 'Pagamento de juros')) or (eventos_debenture_cdi.loc[i - 1, 'evento'] == 'Premio'):
        # Update the current row's start_date with the previous row's start_date
        eventos_debenture_cdi.loc[i,
                                  'start_date'] = eventos_debenture_cdi.loc[i - 1, 'start_date']

# Now, apply the get_busdays function using the start_date and end_date
eventos_debenture_cdi['dp'] = eventos_debenture_cdi.apply(
    lambda row: get_busdays(row['start_date'], row['end_date']) if pd.notnull(
        row['start_date']) else None,
    axis=1
)



# Function to replace 'dp' value with the previous row's value if it's 1 or 0

def use_prev_dp(row, prev_dp):
    if row['dp'] in [0, 1]:
        return prev_dp
    return row['dp']


# Iterate through the DataFrame and adjust 'dp' where necessary
prev_dp = None  # To store the previous row's dp value
for idx in range(len(eventos_debenture_cdi)):
    current_dp = eventos_debenture_cdi.at[idx, 'dp']
    if pd.notnull(current_dp):  # Check if 'dp' is not null
        new_dp = use_prev_dp(eventos_debenture_cdi.iloc[idx], prev_dp)
        eventos_debenture_cdi.at[idx, 'dp'] = new_dp
        prev_dp = new_dp  # Update prev_dp for the next iteration

# Loop over the DataFrame using the index and iterrows()
for i in range(1, len(eventos_debenture_cdi)):  # Start from 1 to avoid going out of bounds
    if eventos_debenture_cdi.loc[i, 'evento'] == 'Amortizacao':
        if eventos_debenture_cdi.loc[i, 'codigo_ticker'] == 'SNGO18':
            # Update 'juros_pagos' for 'Amortizacao' event using the previous row 'pu'
            eventos_debenture_cdi.loc[i, 'juros_pagos'] = (
                eventos_debenture_cdi.loc[i - 1, 'vne'] *
                (eventos_debenture_cdi.loc[i, 'percentual_taxa'] / 100)
            )
        else:
            # Update 'juros_pagos' for 'Amortizacao' event using the previous row 'pu'
            eventos_debenture_cdi.loc[i, 'juros_pagos'] = (
                eventos_debenture_cdi.loc[i - 1, 'pu'] *
                (eventos_debenture_cdi.loc[i, 'percentual_taxa'] / 100)
            )
    elif eventos_debenture_cdi.loc[i, 'evento'] == 'Premio':
        eventos_debenture_cdi.loc[i,
                                  'juros_pagos'] = eventos_debenture_cdi.loc[i, 'valor_pago']


# Apply the function and create the new column 'juros_pagos'
# eventos_debenture_cdi['juros_pagos'] = eventos_debenture_cdi.apply(apply_calcular_juros, axis=1)
# Create a mask for rows where 'juros_pagos' is null
mask = eventos_debenture_cdi['juros_pagos'].isna()
eventos_debenture_cdi.loc[mask, 'juros_pagos'] = eventos_debenture_cdi[mask].apply(
    apply_calcular_juros, axis=1)

# Create a dictionary mapping English month names to Portuguese
month_mapping = {
    'January': 'Janeiro',
    'February': 'Fevereiro',
    'March': 'Março',
    'April': 'Abril',
    'May': 'Maio',
    'June': 'Junho',
    'July': 'Julho',
    'August': 'Agosto',
    'September': 'Setembro',
    'October': 'Outubro',
    'November': 'Novembro',
    'December': 'Dezembro'
}

# Extract the English month names and map them to Portuguese using the dictionary
eventos_debenture_cdi['month_w'] = eventos_debenture_cdi['data_evento'].dt.strftime(
    '%B').map(month_mapping)
eventos_debenture_cdi['year_w'] = eventos_debenture_cdi['data_evento'].dt.year

# sql query to read all the records
posicoes_query = pd.read_sql(
    'SELECT * FROM posicoes_pbi ORDER BY posicao_id', engine)

# convert the SQL table into a pandas dataframe
posicoes_pbi = pd.DataFrame(posicoes_query)

grouped_df = posicoes_pbi[['data', 'codigo_custodia_ticker', 'quantidade', 'fundo']][(posicoes_pbi['tipo_papel_resumido'] == 'DEBENTURE')].groupby(['data', 'codigo_custodia_ticker', 'fundo']).agg({
    'quantidade': 'sum'  # Use sum or any other aggregate function
}).reset_index()


grouped_df['data'] = pd.to_datetime(grouped_df['data'])

max_date = eventos_debenture_cdi['data_evento'].max()
min_date = grouped_df['data'].max()

custom_business_day = CustomBusinessDay(calendar=CustomBusinessCalendar())
new_dates = pd.bdate_range(
    start=min_date, end=max_date, freq=custom_business_day)
new_dates = new_dates[1:]

new_entries = pd.DataFrame({
    'data': new_dates,
    'codigo_custodia_ticker': np.nan,
    'fundo': np.nan,
    'quantidade': [float('nan')] * len(new_dates)
})

current_entries = grouped_df[['codigo_custodia_ticker', 'fundo',
                              'quantidade']][grouped_df['data'] == min_date].reset_index(drop=True)
new_entries = new_entries.reset_index(drop=True)
current_entries = current_entries.reset_index(drop=True)

# Add a key column to both DataFrames for cross join
new_entries['key'] = 1
current_entries['key'] = 1

# Perform the cross join
new_entries = pd.merge(new_entries, current_entries,
                       on='key').drop('key', axis=1)

new_entries = new_entries.drop(
    columns=['codigo_custodia_ticker_x', 'fundo_x', 'quantidade_x'])
new_entries = new_entries.rename(columns={
    'codigo_custodia_ticker_y': 'codigo_custodia_ticker',
    'fundo_y': 'fundo',
    'quantidade_y': 'quantidade'
})

# Append the new entries to indicador_cdi
grouped_df = pd.concat([grouped_df, new_entries], ignore_index=True)

# Sort the DataFrame by 'data' to maintain chronological order
grouped_df.sort_values(by='data', inplace=True)

# Reset index if necessary
grouped_df.reset_index(drop=True, inplace=True)

# Merge eventos_debenture_cdi with grouped_df
eventos_debenture_cdi = eventos_debenture_cdi.merge(
    grouped_df,
    # Columns from eventos_debenture_cdi
    left_on=['data_liquidacao', 'codigo_ticker'],
    right_on=['data', 'codigo_custodia_ticker'],  # Columns from grouped_df
    how='left'  # Choose 'left' to keep all rows from eventos_debenture_cdi
)

# Drop unnecessary columns after merge
eventos_debenture_cdi = eventos_debenture_cdi.drop(
    columns=['data', 'codigo_custodia_ticker'])

eventos_debenture_cdi = eventos_debenture_cdi[eventos_debenture_cdi['data_evento'] >= '01-01-2024']

eventos_debenture_cdi['quantidade'] = eventos_debenture_cdi['quantidade'].fillna(
    method='ffill')
eventos_debenture_cdi['fundo'] = eventos_debenture_cdi['fundo'].fillna(
    method='ffill')

# Create the 'valor_recebido' column with the specified conditions
eventos_debenture_cdi['valor_recebido'] = np.where(
    eventos_debenture_cdi['valor_pago'].notnull(),
    # If 'valor_pago' is not null
    eventos_debenture_cdi['valor_pago'] * eventos_debenture_cdi['quantidade'],
    # If 'valor_pago' is null
    eventos_debenture_cdi['juros_pagos'] * eventos_debenture_cdi['quantidade']
)


eventos_debenture_cdi.reset_index(drop=True, inplace=True)
eventos_debenture_cdi['deb_id'] = eventos_debenture_cdi.index

# Iterate over rows and apply the function
for i in range(1, len(eventos_debenture_cdi)):
    eventos_debenture_cdi.at[i, 'valor_recebido'] = update_valor_recebido(
        eventos_debenture_cdi.iloc[i], eventos_debenture_cdi.iloc[i - 1]
    )




eventos_debenture_cdi


set(eventos_debenture_cdi['codigo_ticker'][eventos_debenture_cdi['inicio_rentabilidade'] >= '2023-01-01'])


eventos_debenture_cdi[eventos_debenture_cdi['codigo_ticker'] == 'BSA317']



# Example: 'postgresql://username:password@localhost:5432/your_database'
engine = create_engine(
    'postgresql://postgres:admin@192.168.88.61:5432/posicoesdb')

# Initialize metadata object
metadata = MetaData()

# Load a table from the database using its name
table_eventos_debenture_cdi = Table(
    'cp_eventos_debenture_di', metadata, autoload_with=engine)

# Drop the table
table_eventos_debenture_cdi.drop(engine)

# Create the table
table_eventos_debenture_cdi.create(engine)

start_time = time.time()  # get start time before insert

eventos_debenture_cdi.set_index('deb_id', inplace=True)

# Subir deb no bd
eventos_debenture_cdi.to_sql(
    name="cp_eventos_debenture_di",  # table name
    con=engine,  # engine
    if_exists="append",  # If the table already exists, append
    index=True  # no index
)

end_time = time.time()  # get end time after insert
total_time = end_time - start_time  # calculate the time
print(f"Insert time: {total_time} seconds")  # print time

