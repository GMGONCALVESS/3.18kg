{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3f9c338b-3e0b-487b-bfb4-f1006d080ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import time\n",
    "import xlrd\n",
    "import requests\n",
    "import csv\n",
    "import math\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine, MetaData, Table\n",
    "from datetime import datetime, date, timedelta\n",
    "from urllib.request import urlretrieve\n",
    "from pandas.tseries.holiday import AbstractHolidayCalendar, Holiday\n",
    "from pandas.tseries.offsets import CustomBusinessDay\n",
    "from openpyxl import Workbook\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "# def truncate(value, decimals=16):\n",
    "#     factor = 10 ** decimals\n",
    "#     return math.trunc(value * factor) / factor\n",
    "def truncate(value, decimals=16):\n",
    "    if isinstance(value, (pd.Timestamp, pd.DatetimeIndex, np.datetime64)):  \n",
    "        # If value is a datetime, return it unchanged (or handle differently if needed)\n",
    "        return value  \n",
    "\n",
    "    factor = 10 ** decimals\n",
    "    return math.trunc(value * factor) / factor\n",
    "\n",
    "# Pega feriados nacionais pelo calendário da Anbima\n",
    "def holidays(url=None, path=None):\n",
    "    if not url:\n",
    "        url = 'http://www.anbima.com.br/feriados/arqs/feriados_nacionais.xls'\n",
    "    if not path:\n",
    "        path = 'feriados_nacionais.xls'\n",
    "    try:\n",
    "        wb = xlrd.open_workbook(path)\n",
    "    except:\n",
    "        response = urlretrieve(url, filename=path)\n",
    "        wb = xlrd.open_workbook(path)\n",
    "    ws = wb.sheet_by_index(0)\n",
    "    i = 1\n",
    "    dates = []\n",
    "    while ws.cell_type(i, 0) == 3:\n",
    "        y, m, d, _, _, _ = xlrd.xldate_as_tuple(\n",
    "            ws.cell_value(i, 0), wb.datemode)\n",
    "        dates.append(date(y, m, d))\n",
    "        i += 1\n",
    "    return dates\n",
    "\n",
    "# Cria calendário de feriados nacionais\n",
    "class CustomBusinessCalendar(AbstractHolidayCalendar):\n",
    "    rules = [Holiday('Brazil Holiday', month=date.month,\n",
    "                     day=date.day, year=date.year) for date in holidays()]\n",
    "\n",
    "\n",
    "def get_busdays(start_date, end_date):\n",
    "    # Cria calendário de dias úteis\n",
    "    custom_business_day = CustomBusinessDay(calendar=CustomBusinessCalendar())\n",
    "    dates = pd.bdate_range(start=start_date, end=end_date,\n",
    "                           freq=custom_business_day)\n",
    "    return len(dates)\n",
    "\n",
    "# Define the function to swap the rows\n",
    "\n",
    "\n",
    "def swap_rows(df):\n",
    "    # Iterate over the DataFrame by index\n",
    "    i = 0\n",
    "    while i < len(df) - 1:\n",
    "        # Check if the 'evento' column has the specific values in the current row\n",
    "        if df.at[i, 'evento'] in ['Amortizacao', 'Vencimento (resgate)']:\n",
    "            # Swap the current row with the next one\n",
    "            df.iloc[i], df.iloc[i+1] = df.iloc[i+1].copy(), df.iloc[i].copy()\n",
    "            # Move the index by 2 to skip the next row (since it's already swapped)\n",
    "            i += 1\n",
    "        i += 1\n",
    "    return df\n",
    "\n",
    "\n",
    "def adjust_pu(row, previous_pu):\n",
    "    if row['evento'] in ['Amortizacao', 'Vencimento (resgate)', 'Resgate total antecipado']:\n",
    "        if row['codigo_ticker'] == 'SNGO18' and row['evento'] == 'Amortizacao':\n",
    "            # print(f'percentual_taxa:{row['percentual_taxa']}; vne:{row['vne']}')\n",
    "            valor_pago = (row['percentual_taxa']/100.0) * row['vne']\n",
    "            return previous_pu - valor_pago\n",
    "        else:\n",
    "            # Adjust using the previous 'pu'\n",
    "            return previous_pu * (1 - row['percentual_taxa'] / 100)\n",
    "       # return previous_pu * (1 - row['percentual_taxa'] / 100)  # Adjust using the previous 'pu'\n",
    "    else:\n",
    "        return previous_pu  # Use the previous 'pu' for other events without adjustment\n",
    "\n",
    "# Step 1: Create a function to find the closest tenor\n",
    "\n",
    "\n",
    "def get_closest_tenor(days, tenores_dict):\n",
    "    # Find the tenor with the minimum difference to 'days'\n",
    "    closest_tenor = min(tenores_dict, key=lambda k: abs(k - days))\n",
    "    return closest_tenor\n",
    "\n",
    "\n",
    "def calcular_juros_acimadi(vne, spread_anual, dp, start_date, end_date):\n",
    "    # Constante: 252 dias úteis no ano\n",
    "    dias_uteis_ano = 252\n",
    "\n",
    "    tdi_k_diario = pd.DataFrame()\n",
    "\n",
    "    # Seleciona o período do DI\n",
    "    tdi_k_diario = indicador_cdi[(indicador_cdi['index'] >= start_date) & (\n",
    "        indicador_cdi['index'] < end_date)]\n",
    "\n",
    "    tdi_k_diario.to_excel('tdi_k_diario.xlsx')\n",
    "\n",
    "    # Calcular a taxa DI diária\n",
    "    tdi_k_diario.loc[:, 'tdi_k_diario'] = (\n",
    "        (1 + tdi_k_diario['px_last']/100) ** (1/dias_uteis_ano) - 1)\n",
    "    # tdi_k_diario.loc[:, 'tdi_k_diario'] = tdi_k_diario['tdi_k_diario'] + 1\n",
    "\n",
    "    print(tdi_k_diario)\n",
    "    print(type(tdi_k_diario))\n",
    "\n",
    "    tdi_k_diario = truncate(tdi_k_diario, 16)\n",
    "    \n",
    "    # Calcular o fator DI\n",
    "    spread_anual = round(spread_anual, 2)\n",
    "    fator_di = np.prod(1 + (tdi_k_diario['tdi_k_diario'] * (spread_anual/100)))\n",
    "\n",
    "    fator_di = round(fator_di, 8)\n",
    "    \n",
    "    # Calcular os juros\n",
    "    juros = vne * (fator_di - 1)\n",
    "\n",
    "    print({\n",
    "        'Fator DI': fator_di,\n",
    "        'Juros Pagos': juros\n",
    "    })\n",
    "\n",
    "    return juros\n",
    "\n",
    "\n",
    "def calcular_juros(vne, spread_anual, dp, start_date, end_date):\n",
    "    # Constante: 252 dias úteis no ano\n",
    "    dias_uteis_ano = 252\n",
    "\n",
    "    vne = truncate(vne, 6)\n",
    "    \n",
    "    tdi_k_diario = pd.DataFrame()\n",
    "\n",
    "    # Seleciona o período do DI\n",
    "    tdi_k_diario = indicador_cdi[(indicador_cdi['index'] >= start_date) & (\n",
    "        indicador_cdi['index'] < end_date)]\n",
    "\n",
    "    tdi_k_diario.to_excel('tdi_k_diario.xlsx')\n",
    "\n",
    "    # Calcular a taxa DI diária\n",
    "    tdi_k_diario.loc[:, 'tdi_k_diario'] = (\n",
    "        1 + tdi_k_diario['px_last']/100) ** (1/dias_uteis_ano) - 1\n",
    "    tdi_k_diario.loc[:, 'tdi_k_diario'] = tdi_k_diario['tdi_k_diario'] + 1\n",
    "\n",
    "    print(tdi_k_diario)\n",
    "    print(type(tdi_k_diario))\n",
    "\n",
    "    tdi_k_diario = truncate(tdi_k_diario, 16)\n",
    "    \n",
    "    # Calcular o fator DI\n",
    "    fator_di = np.prod(tdi_k_diario['tdi_k_diario'])\n",
    "    spread_anual = round(spread_anual, 4)\n",
    "    \n",
    "    fator_di = round(fator_di, 8)\n",
    "    \n",
    "    # Calcular o fator spread\n",
    "    fator_spread = (1 + spread_anual / 100) ** (dp / dias_uteis_ano)\n",
    "    \n",
    "    # Calcular o fator de juros\n",
    "    fator_juros = fator_di * fator_spread\n",
    "\n",
    "    fator_juros = round(fator_juros, 9)\n",
    "    \n",
    "    # Calcular os juros\n",
    "    juros = vne * (fator_juros - 1)\n",
    "\n",
    "    print({\n",
    "        'Fator DI': fator_di,\n",
    "        'Fator Spread': fator_spread,\n",
    "        'Fator Juros': fator_juros,\n",
    "        'Juros Pagos': juros\n",
    "    })\n",
    "\n",
    "    return juros\n",
    "\n",
    "# Function to calculate juros_pagos only for 'Pagamento de Juros' events\n",
    "\n",
    "\n",
    "def apply_calcular_juros(row):\n",
    "    if (row['evento'] == 'Pagamento de juros'):\n",
    "        # Call the calcular_juros function using the appropriate columns\n",
    "        dp = row['dp'] - 1\n",
    "        print(f'evento:{row['evento']}; vne:{row['pu']}; percentual_taxa:{row['percentual_taxa']}; dp:{dp}; start_date:{row['start_date']}; end_date:{row['end_date']}')\n",
    "        if row['percentual_taxa'] >= 100:\n",
    "            return calcular_juros_acimadi(row['pu'], row['percentual_taxa'], dp, row['start_date'], row['end_date'])\n",
    "        else:\n",
    "            return calcular_juros(row['pu'], row['percentual_taxa'], dp, row['start_date'], row['end_date'])\n",
    "    elif row['evento'] in ['Amortizacao', 'Vencimento (resgate)', 'Resgate total antecipado']:\n",
    "        print(f'evento: {row['evento']}; pu:{row['pu']}')\n",
    "        return row['pu']\n",
    "    return None  # Return None for rows that don't match the condition\n",
    "\n",
    "\n",
    "def update_valor_recebido(row, previous_row):\n",
    "    # Check if the event is 'Resgate total antecipado' or 'Vencimento (resgate)'\n",
    "    if row['evento'] in ['Resgate total antecipado', 'Vencimento (resgate)']:\n",
    "        # Get the previous row's 'pu' and multiply by 'quantidade'\n",
    "        return previous_row['pu'] * row['quantidade'] if pd.notnull(previous_row['pu']) else None\n",
    "    # Return the original valor_recebido for other rows\n",
    "    return row['valor_recebido']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "64d789de-4dea-4085-806e-64f48bfa958d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: 'postgresql://username:password@localhost:5432/your_database'\n",
    "engine = create_engine(\n",
    "    'postgresql://postgres:admin@192.168.88.61:5432/posicoesdb')\n",
    "\n",
    "# sql query to read all the records\n",
    "posicoes_query = pd.read_sql(\n",
    "    'SELECT * FROM posicoes_pbi ORDER BY posicao_id', engine)\n",
    "\n",
    "# convert the SQL table into a pandas dataframe\n",
    "posicoes_pbi = pd.DataFrame(posicoes_query)\n",
    "\n",
    "# sql query to read all the records\n",
    "eventos_query = pd.read_sql(\n",
    "    'SELECT * FROM eventos_debenture ORDER BY deb_id', engine)\n",
    "\n",
    "# convert the SQL table into a pandas dataframe\n",
    "eventos_debenture = pd.DataFrame(eventos_query)\n",
    "\n",
    "# sql query to read all the records\n",
    "eventos_cricra_query = pd.read_sql(\n",
    "    'SELECT * FROM eventos_cricra ORDER BY cricra_id', engine)\n",
    "\n",
    "# convert the SQL table into a pandas dataframe\n",
    "eventos_cricra = pd.DataFrame(eventos_cricra_query)\n",
    "\n",
    "# sql query to read all the records\n",
    "curvadi_query = pd.read_sql('SELECT * FROM curva_di ORDER BY di_index', engine)\n",
    "\n",
    "# convert the SQL table into a pandas dataframe\n",
    "curva_di = pd.DataFrame(curvadi_query)\n",
    "curva_di = pd.read_excel(\"curvadi_1902.xlsx\")\n",
    "\n",
    "\n",
    "start_date = '2019-02-26'\n",
    "end_date = '2050-08-26'\n",
    "\n",
    "# Cria calendário de dias úteis\n",
    "custom_business_day = CustomBusinessDay(calendar=CustomBusinessCalendar())\n",
    "dates = pd.bdate_range(start=start_date, end=end_date,\n",
    "                       freq=custom_business_day)\n",
    "\n",
    "eventos_debenture_cdi = eventos_debenture[eventos_debenture['indice'] == 'DI']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "67c2fec3-2462-4a06-b3b1-c341d678aa6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GabrielMariano\\AppData\\Local\\Temp\\ipykernel_38684\\3740158653.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  eventos_debenture_cdi['deb_id'] = eventos_debenture_cdi.index\n",
      "C:\\Users\\GabrielMariano\\AppData\\Local\\Temp\\ipykernel_38684\\3740158653.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  eventos_debenture_cdi['pu'] = eventos_debenture_cdi['vne']\n",
      "C:\\Users\\GabrielMariano\\AppData\\Local\\Temp\\ipykernel_38684\\3740158653.py:52: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  eventos_debenture_cdi['data_evento'] = pd.to_datetime(\n"
     ]
    }
   ],
   "source": [
    "# eventos_debenture_cdi[eventos_debenture['codigo_ticker'] == 'ALGA28']\n",
    "eventos_debenture_cdi.reset_index(drop=True, inplace=True)\n",
    "eventos_debenture_cdi['deb_id'] = eventos_debenture_cdi.index\n",
    "\n",
    "# Apply the swap function\n",
    "eventos_debenture_cdi = swap_rows(eventos_debenture_cdi)\n",
    "\n",
    "# Update the 'percentual_taxa' column to 100 where 'evento' is 'Vencimento (resgate)' or 'Resgate total antecipado'\n",
    "eventos_debenture_cdi.loc[eventos_debenture_cdi['evento'].isin(\n",
    "    ['Vencimento (resgate)', 'Resgate total antecipado']), 'percentual_taxa'] = 100\n",
    "\n",
    "# eventos_debenture_cdi['pu'] = 1000.0\n",
    "eventos_debenture_cdi['pu'] = eventos_debenture_cdi['vne']\n",
    "# Adjust 'pu' using the previous 'pu' value for 'Amortizacao' or 'Vencimento (resgate)'\n",
    "\n",
    "previous_codigo = None\n",
    "previous_pu = None\n",
    "\n",
    "# Iterate through the DataFrame\n",
    "for index, row in eventos_debenture_cdi.iterrows():\n",
    "    if previous_codigo is not None and previous_codigo != row['codigo_ticker']:\n",
    "        # Set 'pu' to 1000.0 when a new 'codigo_ticker' is encountered\n",
    "        # eventos_debenture_cdi.at[index, 'pu'] = 1000.0\n",
    "        # previous_pu = 1000.0  # Reset previous_pu to 1000.0 for the new ticker\n",
    "        eventos_debenture_cdi.at[index, 'pu'] = row['vne']\n",
    "        # Reset previous_pu to 1000.0 for the new ticker\n",
    "        previous_pu = row['vne']\n",
    "    elif previous_pu is not None:\n",
    "        # Adjust or propagate the previous 'pu'\n",
    "        eventos_debenture_cdi.at[index, 'pu'] = adjust_pu(row, previous_pu)\n",
    "\n",
    "    # Update the previous values for 'codigo_ticker' and 'pu'\n",
    "    previous_codigo = row['codigo_ticker']\n",
    "    # Store the updated 'pu'\n",
    "    previous_pu = eventos_debenture_cdi.at[index, 'pu']\n",
    "\n",
    "\n",
    "indicador_brasil_usa_query = pd.read_sql(\n",
    "    \"SELECT * FROM indicadores_brasil_usa WHERE pais = 'Brasil' ORDER BY indicador_id\", engine)\n",
    "\n",
    "indicador_brasil = pd.DataFrame(indicador_brasil_usa_query)\n",
    "\n",
    "# sql query to read all the records\n",
    "indicadorcdi_query = pd.read_sql(\n",
    "    'SELECT * FROM indicador_cdi ORDER BY index', engine)\n",
    "\n",
    "# convert the SQL table into a pandas dataframe\n",
    "indicador_cdi = pd.DataFrame(indicadorcdi_query)\n",
    "\n",
    "\n",
    "# Step 1: Convert the date columns to datetime if necessary\n",
    "eventos_debenture_cdi['data_evento'] = pd.to_datetime(\n",
    "    eventos_debenture_cdi['data_evento'])\n",
    "indicador_cdi['index'] = pd.to_datetime(indicador_cdi['index'])\n",
    "indicador_brasil['data'] = pd.to_datetime(indicador_brasil['data'])\n",
    "\n",
    "# Step 2: Merge the DataFrames based on the date column\n",
    "eventos_debenture_cdi = eventos_debenture_cdi.merge(\n",
    "    indicador_cdi[['index', 'px_last']],  # Select only the necessary columns\n",
    "    left_on='data_evento',  # Merge on the 'data_evento' column in eventos_debenture_cdi\n",
    "    right_on='index',  # Merge on the 'index' column in indicador_cdi\n",
    "    how='left'  # Keep all rows from eventos_debenture_cdi and match with indicador_cdi\n",
    ")\n",
    "\n",
    "# Step 3: Rename the 'px_last' column to 'cdi' and drop the redundant 'index' column\n",
    "eventos_debenture_cdi = eventos_debenture_cdi.rename(\n",
    "    columns={'px_last': 'cdi'})\n",
    "eventos_debenture_cdi = eventos_debenture_cdi.drop(columns=['index'])\n",
    "\n",
    "# Step 2: Merge the DataFrames based on the date column\n",
    "eventos_debenture_cdi = eventos_debenture_cdi.merge(\n",
    "    indicador_brasil[['data', 'cdi']],  # Select only the necessary columns\n",
    "    left_on='data_evento',  # Merge on the 'data_evento' column in eventos_debenture_cdi\n",
    "    right_on='data',  # Merge on the 'index' column in indicador_cdi\n",
    "    how='left'  # Keep all rows from eventos_debenture_cdi and match with indicador_cdi\n",
    ")\n",
    "eventos_debenture_cdi = eventos_debenture_cdi.rename(columns={'cdi_y': 'cdi'})\n",
    "eventos_debenture_cdi = eventos_debenture_cdi.drop(columns=['data', 'cdi_x'])\n",
    "\n",
    "tenores = pd.read_excel('curvadi_1902.xlsx')\n",
    "\n",
    "tenores = tenores.set_index('tenor')['bid_yield'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2e912fb4-3c2f-4ef4-a93e-16878db5dfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "indicador_cdi['index'] = pd.to_datetime(indicador_cdi['index'])\n",
    "\n",
    "max_date = eventos_debenture_cdi['data_evento'].max()\n",
    "min_date = indicador_cdi['index'].max()\n",
    "\n",
    "custom_business_day = CustomBusinessDay(calendar=CustomBusinessCalendar())\n",
    "new_dates = pd.bdate_range(\n",
    "    start=min_date, end=max_date, freq=custom_business_day)\n",
    "new_dates = new_dates[1:]\n",
    "\n",
    "new_entries = pd.DataFrame({\n",
    "    'index': new_dates,\n",
    "    'px_last': [float('nan')] * len(new_dates)\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "837f0c86-195b-4402-9e89-a686d20addd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>px_last</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2002-01-10</td>\n",
       "      <td>19.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2002-01-11</td>\n",
       "      <td>19.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2002-01-14</td>\n",
       "      <td>19.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2002-01-15</td>\n",
       "      <td>19.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2002-01-16</td>\n",
       "      <td>19.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5804</th>\n",
       "      <td>2025-02-14</td>\n",
       "      <td>13.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5805</th>\n",
       "      <td>2025-02-17</td>\n",
       "      <td>13.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5806</th>\n",
       "      <td>2025-02-18</td>\n",
       "      <td>13.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5807</th>\n",
       "      <td>2025-02-19</td>\n",
       "      <td>13.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5808</th>\n",
       "      <td>2025-02-20</td>\n",
       "      <td>13.15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5809 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          index  px_last\n",
       "0    2002-01-10    19.03\n",
       "1    2002-01-11    19.02\n",
       "2    2002-01-14    19.02\n",
       "3    2002-01-15    19.02\n",
       "4    2002-01-16    19.02\n",
       "...         ...      ...\n",
       "5804 2025-02-14    13.15\n",
       "5805 2025-02-17    13.15\n",
       "5806 2025-02-18    13.15\n",
       "5807 2025-02-19    13.15\n",
       "5808 2025-02-20    13.15\n",
       "\n",
       "[5809 rows x 2 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indicador_cdi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e62a02d3-f3c8-499a-bd9a-10a689f827e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>px_last</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-02-21</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-02-24</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-02-25</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-02-26</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-02-27</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6051</th>\n",
       "      <td>2049-04-27</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6052</th>\n",
       "      <td>2049-04-28</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6053</th>\n",
       "      <td>2049-04-29</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6054</th>\n",
       "      <td>2049-04-30</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6055</th>\n",
       "      <td>2049-05-03</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6056 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          index  px_last\n",
       "0    2025-02-21      NaN\n",
       "1    2025-02-24      NaN\n",
       "2    2025-02-25      NaN\n",
       "3    2025-02-26      NaN\n",
       "4    2025-02-27      NaN\n",
       "...         ...      ...\n",
       "6051 2049-04-27      NaN\n",
       "6052 2049-04-28      NaN\n",
       "6053 2049-04-29      NaN\n",
       "6054 2049-04-30      NaN\n",
       "6055 2049-05-03      NaN\n",
       "\n",
       "[6056 rows x 2 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8142fa52-b4e6-46c9-8b0d-1769bd1a122a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the new entries to indicador_cdi\n",
    "indicador_cdi = pd.concat([indicador_cdi, new_entries], ignore_index=True)\n",
    "\n",
    "# Sort the DataFrame by 'data' to maintain chronological order\n",
    "indicador_cdi.sort_values(by='index', inplace=True)\n",
    "\n",
    "# Reset index if necessary\n",
    "indicador_cdi.reset_index(drop=True, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "384fd2b3-a08b-40e7-8c14-01c195ed01b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 13.15,\n",
       " 7: 13.16,\n",
       " 8: 13.16,\n",
       " 13: 13.16,\n",
       " 14: 13.21,\n",
       " 21: 13.35,\n",
       " 22: 13.37,\n",
       " 25: 13.38,\n",
       " 28: 13.41,\n",
       " 29: 13.42,\n",
       " 32: 13.43,\n",
       " 33: 13.44,\n",
       " 36: 13.46,\n",
       " 39: 13.46,\n",
       " 40: 13.46,\n",
       " 54: 13.66,\n",
       " 55: 13.68,\n",
       " 56: 13.69,\n",
       " 61: 13.7,\n",
       " 68: 13.76,\n",
       " 69: 13.77,\n",
       " 71: 13.78,\n",
       " 84: 13.9,\n",
       " 90: 13.94,\n",
       " 91: 13.95,\n",
       " 99: 14.0,\n",
       " 102: 14.01,\n",
       " 113: 14.09,\n",
       " 116: 14.1,\n",
       " 118: 14.11,\n",
       " 120: 14.12,\n",
       " 127: 14.15,\n",
       " 130: 14.16,\n",
       " 131: 14.16,\n",
       " 145: 14.23,\n",
       " 151: 14.26,\n",
       " 153: 14.27,\n",
       " 160: 14.29,\n",
       " 161: 14.3,\n",
       " 162: 14.3,\n",
       " 174: 14.34,\n",
       " 176: 14.35,\n",
       " 180: 14.36,\n",
       " 189: 14.39,\n",
       " 190: 14.39,\n",
       " 193: 14.4,\n",
       " 207: 14.44,\n",
       " 210: 14.45,\n",
       " 211: 14.45,\n",
       " 214: 14.46,\n",
       " 222: 14.48,\n",
       " 223: 14.48,\n",
       " 237: 14.51,\n",
       " 238: 14.51,\n",
       " 242: 14.52,\n",
       " 252: 14.53,\n",
       " 253: 14.54,\n",
       " 256: 14.54,\n",
       " 270: 14.57,\n",
       " 272: 14.57,\n",
       " 281: 14.59,\n",
       " 284: 14.59,\n",
       " 300: 14.62,\n",
       " 301: 14.62,\n",
       " 302: 14.62,\n",
       " 312: 14.64,\n",
       " 313: 14.64,\n",
       " 316: 14.64,\n",
       " 329: 14.66,\n",
       " 330: 14.66,\n",
       " 335: 14.67,\n",
       " 344: 14.68,\n",
       " 347: 14.69,\n",
       " 363: 14.7,\n",
       " 372: 14.71,\n",
       " 389: 14.72,\n",
       " 390: 14.72,\n",
       " 393: 14.72,\n",
       " 396: 14.72,\n",
       " 404: 14.73,\n",
       " 405: 14.73,\n",
       " 419: 14.73,\n",
       " 420: 14.73,\n",
       " 449: 14.73,\n",
       " 452: 14.73,\n",
       " 480: 14.73,\n",
       " 482: 14.73,\n",
       " 496: 14.73,\n",
       " 510: 14.72,\n",
       " 538: 14.7,\n",
       " 540: 14.69,\n",
       " 543: 14.69,\n",
       " 571: 14.68,\n",
       " 572: 14.68,\n",
       " 588: 14.67,\n",
       " 600: 14.66,\n",
       " 601: 14.66,\n",
       " 630: 14.64,\n",
       " 634: 14.64,\n",
       " 662: 14.63,\n",
       " 664: 14.63,\n",
       " 666: 14.62,\n",
       " 683: 14.62,\n",
       " 690: 14.62,\n",
       " 694: 14.61,\n",
       " 720: 14.61,\n",
       " 725: 14.6,\n",
       " 727: 14.6,\n",
       " 750: 14.6,\n",
       " 770: 14.59,\n",
       " 781: 14.58,\n",
       " 810: 14.56,\n",
       " 816: 14.56,\n",
       " 840: 14.55,\n",
       " 861: 14.54,\n",
       " 872: 14.53,\n",
       " 900: 14.52,\n",
       " 907: 14.52,\n",
       " 930: 14.51,\n",
       " 953: 14.5,\n",
       " 960: 14.49,\n",
       " 991: 14.48,\n",
       " 999: 14.48,\n",
       " 1020: 14.47,\n",
       " 1047: 14.46,\n",
       " 1050: 14.45,\n",
       " 1082: 14.45,\n",
       " 1090: 14.45,\n",
       " 1110: 14.44,\n",
       " 1138: 14.44,\n",
       " 1140: 14.44,\n",
       " 1170: 14.44,\n",
       " 1180: 14.44,\n",
       " 1201: 14.45,\n",
       " 1229: 14.45,\n",
       " 1272: 14.46,\n",
       " 1320: 14.46,\n",
       " 1365: 14.46,\n",
       " 1412: 14.46,\n",
       " 1440: 14.46,\n",
       " 1456: 14.45,\n",
       " 1502: 14.45,\n",
       " 1545: 14.46,\n",
       " 1593: 14.47,\n",
       " 1623: 14.47,\n",
       " 1637: 14.47,\n",
       " 1684: 14.47,\n",
       " 1730: 14.48,\n",
       " 1777: 14.49,\n",
       " 1800: 14.49,\n",
       " 1821: 14.49,\n",
       " 1866: 14.5,\n",
       " 1910: 14.5,\n",
       " 1957: 14.5,\n",
       " 2002: 14.5,\n",
       " 2097: 14.51,\n",
       " 2142: 14.51,\n",
       " 2160: 14.51,\n",
       " 2188: 14.51,\n",
       " 2275: 14.52,\n",
       " 2367: 14.52,\n",
       " 2461: 14.52,\n",
       " 2507: 14.52,\n",
       " 2520: 14.52,\n",
       " 2552: 14.52,\n",
       " 2643: 14.51,\n",
       " 2734: 14.51,\n",
       " 2826: 14.5,\n",
       " 2874: 14.5,\n",
       " 2881: 14.5,\n",
       " 2917: 14.49,\n",
       " 3007: 14.48,\n",
       " 3098: 14.47,\n",
       " 3191: 14.46,\n",
       " 3238: 14.45,\n",
       " 3240: 14.45,\n",
       " 3282: 14.45,\n",
       " 3371: 14.45,\n",
       " 3463: 14.45,\n",
       " 3556: 14.45,\n",
       " 3603: 14.45,\n",
       " 3647: 14.44,\n",
       " 3736: 14.43,\n",
       " 3828: 14.41,\n",
       " 3921: 14.39,\n",
       " 3961: 14.39,\n",
       " 3968: 14.39,\n",
       " 4012: 14.38,\n",
       " 4102: 14.37,\n",
       " 4194: 14.36,\n",
       " 4288: 14.35,\n",
       " 4320: 14.34,\n",
       " 4334: 14.34,\n",
       " 4381: 14.33,\n",
       " 4467: 14.32,\n",
       " 4561: 14.3,\n",
       " 4652: 14.28,\n",
       " 4680: 14.28,\n",
       " 4701: 14.27,\n",
       " 4743: 14.29,\n",
       " 4834: 14.31,\n",
       " 4925: 14.34,\n",
       " 5017: 14.36,\n",
       " 5040: 14.37,\n",
       " 5065: 14.38,\n",
       " 5108: 14.37,\n",
       " 5198: 14.35,\n",
       " 5289: 14.33,\n",
       " 5382: 14.31,\n",
       " 5401: 14.3,\n",
       " 5429: 14.3,\n",
       " 5473: 14.29,\n",
       " 5563: 14.27,\n",
       " 5580: 14.27,\n",
       " 5655: 14.25,\n",
       " 5748: 14.24,\n",
       " 5839: 14.22,\n",
       " 5928: 14.21,\n",
       " 6020: 14.19,\n",
       " 6115: 14.18,\n",
       " 6208: 14.16,\n",
       " 6293: 14.15,\n",
       " 6385: 14.14,\n",
       " 6479: 14.12,\n",
       " 6570: 14.11,\n",
       " 6658: 14.1,\n",
       " 6752: 14.09,\n",
       " 6843: 14.08,\n",
       " 6934: 14.06,\n",
       " 7025: 14.05,\n",
       " 7116: 14.04,\n",
       " 7200: 14.03,\n",
       " 7209: 14.03,\n",
       " 7300: 14.02,\n",
       " 7389: 14.01,\n",
       " 7481: 14.0,\n",
       " 7665: 13.98,\n",
       " 7846: 13.97,\n",
       " 8030: 13.95,\n",
       " 8211: 13.93,\n",
       " 8399: 13.92,\n",
       " 8579: 13.9,\n",
       " 8761: 13.89,\n",
       " 8943: 13.88,\n",
       " 9001: 13.87,\n",
       " 9126: 13.86,\n",
       " 9307: 13.85,\n",
       " 10800: 13.76,\n",
       " 11043: 13.75,\n",
       " 12961: 13.67}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tenores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b28c0ead-4dad-412f-a85a-4979e6a1e448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from IPython.core.display import display, HTML\n",
    "\n",
    "# # Set the max height and enable scrolling\n",
    "# def display_scrollable(df, max_height=400, max_width=1000):\n",
    "#     display(HTML(df.to_html(classes=\"scroll-table\")))\n",
    "\n",
    "#     # Apply CSS to make it scrollable\n",
    "#     display(HTML(f\"\"\"\n",
    "#     <style>\n",
    "#         .scroll-table {{\n",
    "#             max-height: {max_height}px;\n",
    "#             max-width: {max_width}px;\n",
    "#             overflow: auto;\n",
    "#             display: block;\n",
    "#             white-space: nowrap;\n",
    "#         }}\n",
    "#     </style>\n",
    "#     \"\"\"))\n",
    "\n",
    "# # Example usage\n",
    "# display_scrollable(indicador_cdi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "663bc67d-eb2b-46e2-a972-c7a094a8d50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "indicador_cdi['days'] = (indicador_cdi['index'] - datetime.today()).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5efc4cfb-ae1a-4e7b-9776-b74e50c6e56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "indicador_cdi['tenor'] = indicador_cdi['days'].apply(\n",
    "    lambda x: get_closest_tenor(x, tenores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "aacaa2c2-3226-4b5b-970f-343c3b0f0fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "indicador_cdi = indicador_cdi.merge(\n",
    "    curva_di[['tenor', 'bid_yield']], on='tenor', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5016f333-5f9a-4b0c-95a5-a1126c3d8252",
   "metadata": {},
   "outputs": [],
   "source": [
    "indicador_cdi['px_last'] = indicador_cdi['px_last'].fillna(\n",
    "    indicador_cdi['bid_yield'])\n",
    "indicador_cdi = indicador_cdi.drop(columns=['bid_yield'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "76c30238-5567-4fe0-a304-101856adf040",
   "metadata": {},
   "outputs": [],
   "source": [
    "eventos_debenture_cdi['days'] = (\n",
    "    eventos_debenture_cdi['data_evento'] - datetime.today()).dt.days\n",
    "\n",
    "# Step 2: Apply the function to create a new 'tenor' column\n",
    "eventos_debenture_cdi['tenor'] = eventos_debenture_cdi['days'].apply(\n",
    "    lambda x: get_closest_tenor(x, tenores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b8d3e758-f16d-45f8-8879-b367d541e367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, perform the merge based on the 'tenor' column\n",
    "eventos_debenture_cdi = eventos_debenture_cdi.merge(\n",
    "    curva_di[['tenor', 'bid_yield']], on='tenor', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a0335ffc-2b7b-432a-8491-6ba24c61a347",
   "metadata": {},
   "outputs": [],
   "source": [
    "eventos_debenture_cdi['cdi'] = eventos_debenture_cdi['cdi'].fillna(\n",
    "    eventos_debenture_cdi['bid_yield'])\n",
    "eventos_debenture_cdi = eventos_debenture_cdi.drop(columns=['bid_yield'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "176976fd-2feb-45e6-b4dc-fe616aee854f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, create a new column for the 'end_date' by shifting 'data_evento' column up by one row\n",
    "eventos_debenture_cdi['start_date'] = eventos_debenture_cdi['data_evento'].shift(\n",
    "    1)\n",
    "eventos_debenture_cdi['end_date'] = eventos_debenture_cdi['data_evento']\n",
    "\n",
    "eventos_debenture_cdi.loc[eventos_debenture_cdi['start_date'].isnull(\n",
    "), 'start_date'] = eventos_debenture_cdi['inicio_rentabilidade']\n",
    "\n",
    "# Create a boolean Series where the current 'codigo_ticker' is different from the previous one\n",
    "codigo_change = eventos_debenture_cdi['codigo_ticker'] != eventos_debenture_cdi['codigo_ticker'].shift(\n",
    "    1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0c8ad58c-138a-45a3-9cbe-6e9c2dbf7e00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#DEMORADO\n",
    "# Use .loc to update 'start_date' where 'codigo_ticker' changes\n",
    "eventos_debenture_cdi.loc[codigo_change,\n",
    "                          'start_date'] = eventos_debenture_cdi['inicio_rentabilidade']\n",
    "\n",
    "# Iterate over the rows of the DataFrame\n",
    "for i in range(1, len(eventos_debenture_cdi)):  # Start from 1 to avoid accessing index -1\n",
    "    # Check if start_date is equal to end_date\n",
    "    if ((eventos_debenture_cdi.loc[i, 'start_date'] == eventos_debenture_cdi.loc[i, 'end_date']) & (eventos_debenture_cdi.loc[i, 'evento'] == 'Pagamento de juros')) or (eventos_debenture_cdi.loc[i - 1, 'evento'] == 'Premio'):\n",
    "        # Update the current row's start_date with the previous row's start_date\n",
    "        eventos_debenture_cdi.loc[i,\n",
    "                                  'start_date'] = eventos_debenture_cdi.loc[i - 1, 'start_date']\n",
    "\n",
    "# Now, apply the get_busdays function using the start_date and end_date\n",
    "eventos_debenture_cdi['dp'] = eventos_debenture_cdi.apply(\n",
    "    lambda row: get_busdays(row['start_date'], row['end_date']) if pd.notnull(\n",
    "        row['start_date']) else None,\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fbe7ef5d-ffb2-4061-a223-2e46ac1440e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evento:Pagamento de juros; vne:1000.0; percentual_taxa:2.1; dp:108; start_date:2024-05-07 00:00:00; end_date:2024-10-07 00:00:00\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot perform __mul__ with this index type: DatetimeArray",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 42\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Apply the function and create the new column 'juros_pagos'\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# eventos_debenture_cdi['juros_pagos'] = eventos_debenture_cdi.apply(apply_calcular_juros, axis=1)\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Create a mask for rows where 'juros_pagos' is null\u001b[39;00m\n\u001b[0;32m     41\u001b[0m mask \u001b[38;5;241m=\u001b[39m eventos_debenture_cdi[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjuros_pagos\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misna()\n\u001b[1;32m---> 42\u001b[0m eventos_debenture_cdi\u001b[38;5;241m.\u001b[39mloc[mask, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjuros_pagos\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m eventos_debenture_cdi[mask]\u001b[38;5;241m.\u001b[39mapply(\n\u001b[0;32m     43\u001b[0m     apply_calcular_juros, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Create a dictionary mapping English month names to Portuguese\u001b[39;00m\n\u001b[0;32m     46\u001b[0m month_mapping \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJanuary\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJaneiro\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFebruary\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFevereiro\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDecember\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDezembro\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     59\u001b[0m }\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:10374\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m  10360\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[0;32m  10362\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[0;32m  10363\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m  10364\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10372\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m  10373\u001b[0m )\n\u001b[1;32m> 10374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mapply()\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:916\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw(engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_kwargs)\n\u001b[1;32m--> 916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1063\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1063\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_generator()\n\u001b[0;32m   1064\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1065\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_numba()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1081\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1079\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[0;32m   1080\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[1;32m-> 1081\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(v, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m   1082\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m   1083\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[23], line 197\u001b[0m, in \u001b[0;36mapply_calcular_juros\u001b[1;34m(row)\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m calcular_juros_acimadi(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpu\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpercentual_taxa\u001b[39m\u001b[38;5;124m'\u001b[39m], dp, row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_date\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend_date\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m calcular_juros(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpu\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpercentual_taxa\u001b[39m\u001b[38;5;124m'\u001b[39m], dp, row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_date\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend_date\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevento\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAmortizacao\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVencimento (resgate)\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mResgate total antecipado\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevento: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevento\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; pu:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpu\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[23], line 158\u001b[0m, in \u001b[0;36mcalcular_juros\u001b[1;34m(vne, spread_anual, dp, start_date, end_date)\u001b[0m\n\u001b[0;32m    154\u001b[0m tdi_k_diario\u001b[38;5;241m.\u001b[39mloc[:, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtdi_k_diario\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m tdi_k_diario[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpx_last\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m100\u001b[39m) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39mdias_uteis_ano) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    156\u001b[0m tdi_k_diario\u001b[38;5;241m.\u001b[39mloc[:, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtdi_k_diario\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m tdi_k_diario[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtdi_k_diario\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 158\u001b[0m tdi_k_diario \u001b[38;5;241m=\u001b[39m truncate(tdi_k_diario, \u001b[38;5;241m16\u001b[39m)\n\u001b[0;32m    160\u001b[0m \u001b[38;5;66;03m# Calcular o fator DI\u001b[39;00m\n\u001b[0;32m    161\u001b[0m fator_di \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mprod(tdi_k_diario[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtdi_k_diario\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[23], line 28\u001b[0m, in \u001b[0;36mtruncate\u001b[1;34m(value, decimals)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value  \n\u001b[0;32m     27\u001b[0m factor \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m decimals\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m math\u001b[38;5;241m.\u001b[39mtrunc(value \u001b[38;5;241m*\u001b[39m factor) \u001b[38;5;241m/\u001b[39m factor\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[0;32m     74\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[1;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m method(\u001b[38;5;28mself\u001b[39m, other)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\arraylike.py:202\u001b[0m, in \u001b[0;36mOpsMixin.__mul__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__mul__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__mul__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m--> 202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_arith_method(other, operator\u001b[38;5;241m.\u001b[39mmul)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:7913\u001b[0m, in \u001b[0;36mDataFrame._arith_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   7910\u001b[0m \u001b[38;5;28mself\u001b[39m, other \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_align_for_op(other, axis, flex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, level\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m   7912\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(\u001b[38;5;28mall\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 7913\u001b[0m     new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch_frame_op(other, op, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[0;32m   7914\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(new_data)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:7945\u001b[0m, in \u001b[0;36mDataFrame._dispatch_frame_op\u001b[1;34m(self, right, func, axis)\u001b[0m\n\u001b[0;32m   7942\u001b[0m right \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mitem_from_zerodim(right)\n\u001b[0;32m   7943\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_list_like(right):\n\u001b[0;32m   7944\u001b[0m     \u001b[38;5;66;03m# i.e. scalar, faster than checking np.ndim(right) == 0\u001b[39;00m\n\u001b[1;32m-> 7945\u001b[0m     bm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mapply(array_op, right\u001b[38;5;241m=\u001b[39mright)\n\u001b[0;32m   7946\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(bm, axes\u001b[38;5;241m=\u001b[39mbm\u001b[38;5;241m.\u001b[39maxes)\n\u001b[0;32m   7948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(right, DataFrame):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:361\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[1;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[0;32m    358\u001b[0m             kwargs[k] \u001b[38;5;241m=\u001b[39m obj[b\u001b[38;5;241m.\u001b[39mmgr_locs\u001b[38;5;241m.\u001b[39mindexer]\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(f):\n\u001b[1;32m--> 361\u001b[0m     applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    363\u001b[0m     applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(b, f)(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:393\u001b[0m, in \u001b[0;36mBlock.apply\u001b[1;34m(self, func, **kwargs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Block]:\n\u001b[0;32m    389\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;124;03m    apply the function to my values; return a block if we are not\u001b[39;00m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;124;03m    one\u001b[39;00m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 393\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    395\u001b[0m     result \u001b[38;5;241m=\u001b[39m maybe_coerce_values(result)\n\u001b[0;32m    396\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split_op_result(result)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\array_ops.py:273\u001b[0m, in \u001b[0;36marithmetic_op\u001b[1;34m(left, right, op)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;66;03m# NB: We assume that extract_array and ensure_wrapped_if_datetimelike\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;66;03m#  have already been called on `left` and `right`,\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;66;03m#  and `maybe_prepare_scalar_for_op` has already been called on `right`\u001b[39;00m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# We need to special-case datetime64/timedelta64 dtypes (e.g. because numpy\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# casts integer dtypes to timedelta64 when operating with timedelta64 - GH#22390)\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    267\u001b[0m     should_extension_dispatch(left, right)\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(right, (Timedelta, BaseOffset, Timestamp))\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;66;03m# Timedelta/Timestamp and other custom scalars are included in the check\u001b[39;00m\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;66;03m# because numexpr will fail on it, see GH#31457\u001b[39;00m\n\u001b[1;32m--> 273\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m op(left, right)\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    275\u001b[0m     \u001b[38;5;66;03m# TODO we should handle EAs consistently and move this check before the if/else\u001b[39;00m\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;66;03m# (https://github.com/pandas-dev/pandas/issues/41165)\u001b[39;00m\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;66;03m# error: Argument 2 to \"_bool_arith_check\" has incompatible type\u001b[39;00m\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;66;03m# \"Union[ExtensionArray, ndarray[Any, Any]]\"; expected \"ndarray[Any, Any]\"\u001b[39;00m\n\u001b[0;32m    279\u001b[0m     _bool_arith_check(op, left, right)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[0;32m     74\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[1;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m method(\u001b[38;5;28mself\u001b[39m, other)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\invalid.py:59\u001b[0m, in \u001b[0;36mmake_invalid_op.<locals>.invalid_op\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvalid_op\u001b[39m(\u001b[38;5;28mself\u001b[39m, other\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     58\u001b[0m     typ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot perform \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with this index type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtyp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot perform __mul__ with this index type: DatetimeArray"
     ]
    }
   ],
   "source": [
    "# Function to replace 'dp' value with the previous row's value if it's 1 or 0\n",
    "\n",
    "def use_prev_dp(row, prev_dp):\n",
    "    if row['dp'] in [0, 1]:\n",
    "        return prev_dp\n",
    "    return row['dp']\n",
    "\n",
    "\n",
    "# Iterate through the DataFrame and adjust 'dp' where necessary\n",
    "prev_dp = None  # To store the previous row's dp value\n",
    "for idx in range(len(eventos_debenture_cdi)):\n",
    "    current_dp = eventos_debenture_cdi.at[idx, 'dp']\n",
    "    if pd.notnull(current_dp):  # Check if 'dp' is not null\n",
    "        new_dp = use_prev_dp(eventos_debenture_cdi.iloc[idx], prev_dp)\n",
    "        eventos_debenture_cdi.at[idx, 'dp'] = new_dp\n",
    "        prev_dp = new_dp  # Update prev_dp for the next iteration\n",
    "\n",
    "# Loop over the DataFrame using the index and iterrows()\n",
    "for i in range(1, len(eventos_debenture_cdi)):  # Start from 1 to avoid going out of bounds\n",
    "    if eventos_debenture_cdi.loc[i, 'evento'] == 'Amortizacao':\n",
    "        if eventos_debenture_cdi.loc[i, 'codigo_ticker'] == 'SNGO18':\n",
    "            # Update 'juros_pagos' for 'Amortizacao' event using the previous row 'pu'\n",
    "            eventos_debenture_cdi.loc[i, 'juros_pagos'] = (\n",
    "                eventos_debenture_cdi.loc[i - 1, 'vne'] *\n",
    "                (eventos_debenture_cdi.loc[i, 'percentual_taxa'] / 100)\n",
    "            )\n",
    "        else:\n",
    "            # Update 'juros_pagos' for 'Amortizacao' event using the previous row 'pu'\n",
    "            eventos_debenture_cdi.loc[i, 'juros_pagos'] = (\n",
    "                eventos_debenture_cdi.loc[i - 1, 'pu'] *\n",
    "                (eventos_debenture_cdi.loc[i, 'percentual_taxa'] / 100)\n",
    "            )\n",
    "    elif eventos_debenture_cdi.loc[i, 'evento'] == 'Premio':\n",
    "        eventos_debenture_cdi.loc[i,\n",
    "                                  'juros_pagos'] = eventos_debenture_cdi.loc[i, 'valor_pago']\n",
    "\n",
    "\n",
    "# Apply the function and create the new column 'juros_pagos'\n",
    "# eventos_debenture_cdi['juros_pagos'] = eventos_debenture_cdi.apply(apply_calcular_juros, axis=1)\n",
    "# Create a mask for rows where 'juros_pagos' is null\n",
    "mask = eventos_debenture_cdi['juros_pagos'].isna()\n",
    "eventos_debenture_cdi.loc[mask, 'juros_pagos'] = eventos_debenture_cdi[mask].apply(\n",
    "    apply_calcular_juros, axis=1)\n",
    "\n",
    "# Create a dictionary mapping English month names to Portuguese\n",
    "month_mapping = {\n",
    "    'January': 'Janeiro',\n",
    "    'February': 'Fevereiro',\n",
    "    'March': 'Março',\n",
    "    'April': 'Abril',\n",
    "    'May': 'Maio',\n",
    "    'June': 'Junho',\n",
    "    'July': 'Julho',\n",
    "    'August': 'Agosto',\n",
    "    'September': 'Setembro',\n",
    "    'October': 'Outubro',\n",
    "    'November': 'Novembro',\n",
    "    'December': 'Dezembro'\n",
    "}\n",
    "\n",
    "# Extract the English month names and map them to Portuguese using the dictionary\n",
    "eventos_debenture_cdi['month_w'] = eventos_debenture_cdi['data_evento'].dt.strftime(\n",
    "    '%B').map(month_mapping)\n",
    "eventos_debenture_cdi['year_w'] = eventos_debenture_cdi['data_evento'].dt.year\n",
    "\n",
    "# sql query to read all the records\n",
    "posicoes_query = pd.read_sql(\n",
    "    'SELECT * FROM posicoes_pbi ORDER BY posicao_id', engine)\n",
    "\n",
    "# convert the SQL table into a pandas dataframe\n",
    "posicoes_pbi = pd.DataFrame(posicoes_query)\n",
    "\n",
    "grouped_df = posicoes_pbi[['data', 'codigo_custodia_ticker', 'quantidade', 'fundo']][(posicoes_pbi['tipo_papel_resumido'] == 'DEBENTURE')].groupby(['data', 'codigo_custodia_ticker', 'fundo']).agg({\n",
    "    'quantidade': 'sum'  # Use sum or any other aggregate function\n",
    "}).reset_index()\n",
    "\n",
    "\n",
    "grouped_df['data'] = pd.to_datetime(grouped_df['data'])\n",
    "\n",
    "max_date = eventos_debenture_cdi['data_evento'].max()\n",
    "min_date = grouped_df['data'].max()\n",
    "\n",
    "custom_business_day = CustomBusinessDay(calendar=CustomBusinessCalendar())\n",
    "new_dates = pd.bdate_range(\n",
    "    start=min_date, end=max_date, freq=custom_business_day)\n",
    "new_dates = new_dates[1:]\n",
    "\n",
    "new_entries = pd.DataFrame({\n",
    "    'data': new_dates,\n",
    "    'codigo_custodia_ticker': np.nan,\n",
    "    'fundo': np.nan,\n",
    "    'quantidade': [float('nan')] * len(new_dates)\n",
    "})\n",
    "\n",
    "current_entries = grouped_df[['codigo_custodia_ticker', 'fundo',\n",
    "                              'quantidade']][grouped_df['data'] == min_date].reset_index(drop=True)\n",
    "new_entries = new_entries.reset_index(drop=True)\n",
    "current_entries = current_entries.reset_index(drop=True)\n",
    "\n",
    "# Add a key column to both DataFrames for cross join\n",
    "new_entries['key'] = 1\n",
    "current_entries['key'] = 1\n",
    "\n",
    "# Perform the cross join\n",
    "new_entries = pd.merge(new_entries, current_entries,\n",
    "                       on='key').drop('key', axis=1)\n",
    "\n",
    "new_entries = new_entries.drop(\n",
    "    columns=['codigo_custodia_ticker_x', 'fundo_x', 'quantidade_x'])\n",
    "new_entries = new_entries.rename(columns={\n",
    "    'codigo_custodia_ticker_y': 'codigo_custodia_ticker',\n",
    "    'fundo_y': 'fundo',\n",
    "    'quantidade_y': 'quantidade'\n",
    "})\n",
    "\n",
    "# Append the new entries to indicador_cdi\n",
    "grouped_df = pd.concat([grouped_df, new_entries], ignore_index=True)\n",
    "\n",
    "# Sort the DataFrame by 'data' to maintain chronological order\n",
    "grouped_df.sort_values(by='data', inplace=True)\n",
    "\n",
    "# Reset index if necessary\n",
    "grouped_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Merge eventos_debenture_cdi with grouped_df\n",
    "eventos_debenture_cdi = eventos_debenture_cdi.merge(\n",
    "    grouped_df,\n",
    "    # Columns from eventos_debenture_cdi\n",
    "    left_on=['data_liquidacao', 'codigo_ticker'],\n",
    "    right_on=['data', 'codigo_custodia_ticker'],  # Columns from grouped_df\n",
    "    how='left'  # Choose 'left' to keep all rows from eventos_debenture_cdi\n",
    ")\n",
    "\n",
    "# Drop unnecessary columns after merge\n",
    "eventos_debenture_cdi = eventos_debenture_cdi.drop(\n",
    "    columns=['data', 'codigo_custodia_ticker'])\n",
    "\n",
    "eventos_debenture_cdi = eventos_debenture_cdi[eventos_debenture_cdi['data_evento'] >= '01-01-2024']\n",
    "\n",
    "eventos_debenture_cdi['quantidade'] = eventos_debenture_cdi['quantidade'].fillna(\n",
    "    method='ffill')\n",
    "eventos_debenture_cdi['fundo'] = eventos_debenture_cdi['fundo'].fillna(\n",
    "    method='ffill')\n",
    "\n",
    "# Create the 'valor_recebido' column with the specified conditions\n",
    "eventos_debenture_cdi['valor_recebido'] = np.where(\n",
    "    eventos_debenture_cdi['valor_pago'].notnull(),\n",
    "    # If 'valor_pago' is not null\n",
    "    eventos_debenture_cdi['valor_pago'] * eventos_debenture_cdi['quantidade'],\n",
    "    # If 'valor_pago' is null\n",
    "    eventos_debenture_cdi['juros_pagos'] * eventos_debenture_cdi['quantidade']\n",
    ")\n",
    "\n",
    "\n",
    "eventos_debenture_cdi.reset_index(drop=True, inplace=True)\n",
    "eventos_debenture_cdi['deb_id'] = eventos_debenture_cdi.index\n",
    "\n",
    "# Iterate over rows and apply the function\n",
    "for i in range(1, len(eventos_debenture_cdi)):\n",
    "    eventos_debenture_cdi.at[i, 'valor_recebido'] = update_valor_recebido(\n",
    "        eventos_debenture_cdi.iloc[i], eventos_debenture_cdi.iloc[i - 1]\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137e44f9-d12c-4f98-83a5-b0cfd68fbf64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eventos_debenture_cdi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7398018-328a-4966-9751-74086659103f",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(eventos_debenture_cdi['codigo_ticker'][eventos_debenture_cdi['inicio_rentabilidade'] >= '2023-01-01'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ae2715-c9ba-43cd-80fb-bcd4e7d66f7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eventos_debenture_cdi[eventos_debenture_cdi['codigo_ticker'] == 'BSA317']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b59fd4-0958-4976-9e6b-624829b2eaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example: 'postgresql://username:password@localhost:5432/your_database'\n",
    "engine = create_engine(\n",
    "    'postgresql://postgres:admin@192.168.88.61:5432/posicoesdb')\n",
    "\n",
    "# Initialize metadata object\n",
    "metadata = MetaData()\n",
    "\n",
    "# Load a table from the database using its name\n",
    "table_eventos_debenture_cdi = Table(\n",
    "    'cp_eventos_debenture_di', metadata, autoload_with=engine)\n",
    "\n",
    "# Drop the table\n",
    "table_eventos_debenture_cdi.drop(engine)\n",
    "\n",
    "# Create the table\n",
    "table_eventos_debenture_cdi.create(engine)\n",
    "\n",
    "start_time = time.time()  # get start time before insert\n",
    "\n",
    "eventos_debenture_cdi.set_index('deb_id', inplace=True)\n",
    "\n",
    "# Subir deb no bd\n",
    "eventos_debenture_cdi.to_sql(\n",
    "    name=\"cp_eventos_debenture_di\",  # table name\n",
    "    con=engine,  # engine\n",
    "    if_exists=\"append\",  # If the table already exists, append\n",
    "    index=True  # no index\n",
    ")\n",
    "\n",
    "end_time = time.time()  # get end time after insert\n",
    "total_time = end_time - start_time  # calculate the time\n",
    "print(f\"Insert time: {total_time} seconds\")  # print time\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
